{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_LOCbj3WuCX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "j2inQWscC9ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sample_dataset = tfds.load(\"imagenet_resized\", split=\"train\", try_gcs=True)\n",
        "Sample_dataset"
      ],
      "metadata": {
        "id": "AMuKWYQADhLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(Sample_dataset, tf.data.Dataset)\n",
        "Sample_dataset"
      ],
      "metadata": {
        "id": "s120WD_lD2Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ6yR2S_Wzi4"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "def preprocess_images(images):\n",
        "  images = images.reshape((images.shape[0], 32,32,3)) / 255.\n",
        "  return images\n",
        "\n",
        "train_images = preprocess_images(train_images)\n",
        "test_images = preprocess_images(test_images)\n",
        "\n",
        "train_size = 50000\n",
        "batch_size = 100\n",
        "test_size = 10000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVKvRW_wajdY"
      },
      "outputs": [],
      "source": [
        "train_dataset = (tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "                 .shuffle(train_size,reshuffle_each_iteration=True).batch(batch_size,drop_remainder=True))\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices((test_images,test_labels))\n",
        "                .shuffle(test_size).batch(batch_size,drop_remainder=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6E62VVW_H76P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ML8ObJFcMb9"
      },
      "source": [
        "### prepare model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odXhTDpr37eO"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomDropout(tf.keras.layers.Layer):\n",
        "    def __init__(self, rate, input_dim, **kwargs):\n",
        "        super(CustomDropout, self).__init__(**kwargs)\n",
        "        self.rate = 1-rate\n",
        "        self.input_dim = input_dim\n",
        "        self.mask_w = self.add_weight(shape=(self.input_dim,n_decision_makers), trainable=True)\n",
        "        self.mask_b = self.add_weight(shape=(n_decision_makers,), initializer=\"zeros\",trainable=True)\n",
        "\n",
        "    def call(self, inputs, label, training=None):\n",
        "        if training:\n",
        "          scce = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
        "          loss = scce(tf.tile(tf.transpose([label],perm = [1,2,0]),[1,n_decision_makers,1]),inputs)\n",
        "          threshold = tfp.stats.percentile(loss, q=self.rate*100)\n",
        "          dropout_mask = (loss<=threshold) ## <= 1-rate keep the best 10%\n",
        "          mask = tf.tile(tf.expand_dims(dropout_mask, axis=-1), [1,1,10])\n",
        "          mask_pred = tf.nn.sigmoid(tf.matmul(tf.keras.layers.Flatten()(inputs), self.mask_w)+self.mask_b)\n",
        "          mask_pred = tf.tile(mask_pred, [1,10])\n",
        "          return tf.multiply(tf.keras.layers.Reshape((n_decision_makers,10))(mask_pred), inputs), tf.cast(mask,'float32'), mask_pred\n",
        "        else:\n",
        "          mask_pred = tf.nn.sigmoid(tf.matmul(tf.keras.layers.Flatten()(inputs), self.mask_w)+self.mask_b)\n",
        "          mask_pred = tf.tile(mask_pred, [1,10])\n",
        "          return tf.multiply(tf.keras.layers.Reshape((n_decision_makers,10))(mask_pred),inputs),tf.ones(shape = (batch_size,n_decision_makers,10)),mask_pred ## reshape self.mask"
      ],
      "metadata": {
        "id": "7aX2MUM78BDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JiJRzgdHSaP"
      },
      "outputs": [],
      "source": [
        "n_decision_makers = 5  #100\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self,**kwargs):\n",
        "      super(MyModel,self).__init__(**kwargs)\n",
        "      model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "# Adding a Global Average Pooling layer\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "# Adding dense layers\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "      self.flat1 = tf.keras.layers.Flatten()\n",
        "      self.flat2 = tf.keras.layers.Flatten()\n",
        "      self.flat3 = tf.keras.layers.Flatten()\n",
        "      self.flat4 = tf.keras.layers.Flatten()\n",
        "      self.flat5 = tf.keras.layers.Flatten()\n",
        "      self.flat6 = tf.keras.layers.Flatten()\n",
        "      self.reshape1 = tf.keras.layers.Reshape((n_decision_makers,10))\n",
        "      self.reshape2 = tf.keras.layers.Reshape((n_decision_makers,10))\n",
        "\n",
        "      self.dropout1 = CustomDropout(0.1,n_decision_makers*10)\n",
        "      self.dropout4 = tf.keras.layers.Dropout(0.2)\n",
        "      self.dropout5 = tf.keras.layers.Dropout(0.2)\n",
        "\n",
        "      self.pool1 = tf.keras.layers.MaxPooling2D((2, 2))\n",
        "      self.pool2 = tf.keras.layers.MaxPooling2D((2, 2))\n",
        "\n",
        "      self.conv1 = tf.keras.layers.Conv2D(96, 5, activation='relu',padding='same',kernel_regularizer=tf.keras.regularizers.l1(l=0.01),kernel_initializer='he_uniform',)\n",
        "      self.conv11 = tf.keras.layers.Conv2D(128, 5, activation='relu',padding='same',kernel_regularizer=tf.keras.regularizers.l1(l=0.01),kernel_initializer='he_uniform',)\n",
        "      self.dense1 = tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "      self.batchnorm1 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "      self.conv2 = tf.keras.layers.Conv2D(256, 5, activation='relu',padding='same',kernel_regularizer=tf.keras.regularizers.l1(l=0.01),kernel_initializer='he_uniform',)\n",
        "      self.conv22 = tf.keras.layers.Conv2D(82, 5, activation='relu',padding='same',kernel_regularizer=tf.keras.regularizers.l1(l=0.01),kernel_initializer='he_uniform',)\n",
        "      self.dense2 = tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "      self.batchnorm2 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "      self.dense5 = tf.keras.layers.Dense(n_decision_makers*10,activation=tf.nn.relu)\n",
        "      self.dense7 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\n",
        "    def call(self, input):\n",
        "\n",
        "      [input, label] = input\n",
        "      hidden_conv1 = self.dropout4(self.batchnorm1(self.pool1(self.conv1(self.conv11(input)))))\n",
        "      hidden_conv1_reshape = self.flat4(hidden_conv1)\n",
        "      hidden_conv1_out = self.dense1(hidden_conv1_reshape)\n",
        "\n",
        "      hidden_conv2 = self.dropout5(self.batchnorm2(self.pool2(self.conv2((hidden_conv1)))))\n",
        "      hidden_conv2_reshape = self.flat5(hidden_conv2)\n",
        "      hidden_conv2_out = self.dense2(hidden_conv2_reshape)\n",
        "\n",
        "      hidden1 = self.dense5(hidden_conv2_reshape)\n",
        "      hidden1_reshape = self.reshape1(hidden1)\n",
        "      hidden1_softmax = tf.nn.softmax(hidden1_reshape)\n",
        "      hidden1_out,hidden1_true_mask,hidden1_pred_mask = self.dropout1(hidden1_softmax,label)\n",
        "\n",
        "      outputs = self.dense7(self.flat1(hidden1_out))\n",
        "\n",
        "      return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY4Lp6VdY80r"
      },
      "outputs": [],
      "source": [
        "model = MyModel()\n",
        "model([tf.zeros((batch_size, 32,32,3)),tf.zeros((batch_size, 1))])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUHrwiGB3iss"
      },
      "outputs": [],
      "source": [
        "\n",
        "for layer in model.layers:\n",
        "    print(layer.name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc=tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "\t optimizer='adam',\n",
        "\t metrics=acc)\n",
        "\n",
        "model.fit([train_images, train_labels], train_labels,\n",
        "\t validation_data=([test_images, np.zeros(test_labels.shape)],test_labels),\n",
        "\t epochs=100,\n",
        "\t batch_size=batch_size,\n",
        ")"
      ],
      "metadata": {
        "id": "6vsWNUPto-mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.shape"
      ],
      "metadata": {
        "id": "geTUMkmUTnAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS0rYDLREvkI"
      },
      "source": [
        "### Local loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaUaKSEW98Ea"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "\n",
        "\n",
        "train_loss, test_loss = tf.keras.metrics.Mean(),tf.keras.metrics.Mean()\n",
        "mask_loss = tf.keras.metrics.Mean()\n",
        "train_acc = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
        "test_acc = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')\n",
        "\n",
        "\n",
        "initial_learning_rate = 1e-4\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "optimizer_mask = tf.keras.optimizers.Adam()\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = 5e-4)\n",
        "\n",
        "\n",
        "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "cce = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def compute_loss(hidden1_true_mask, hidden1_pred_mask, conv1, conv2, hidden1, y, output):\n",
        "  loss_local_conv1 = scce(y,conv1)\n",
        "  loss_local_conv2 = scce(y,conv2)\n",
        "\n",
        "  loss_local_hidden1 = scce(tf.tile(y, [1,n_decision_makers]),hidden1)\n",
        "\n",
        "  loss = scce(y,output)\n",
        "\n",
        "  loss_mask = cce(hidden1_true_mask, hidden1_pred_mask)\n",
        "\n",
        "  return loss, loss_mask, loss_local_conv1, loss_local_conv2, loss_local_hidden1\n",
        "\n",
        "def compute_loss_mask(hidden1_true_mask, hidden1_pred_mask):\n",
        "  loss_mask = cce(hidden1_true_mask, hidden1_pred_mask)\n",
        "\n",
        "  return loss_mask\n",
        "\n",
        "def compute_acc(model, x, y):\n",
        "  _,_,_,_,_,output = model([x,y])\n",
        "  acc = tf.keras.metrics.sparse_categorical_accuracy(y, output)\n",
        "  return acc\n",
        "\n",
        "def train_step(model, x, y, optimizer):\n",
        "  with tf.GradientTape(persistent =True) as tape:\n",
        "    hidden1_true_mask, hidden1_pred_mask, conv1, conv2, hidden1, output = model([x,y], training=True)\n",
        "    loss, loss_mask, loss_local_conv1, loss_local_conv2, loss_local_hidden1 = compute_loss(hidden1_true_mask, hidden1_pred_mask, conv1, conv2, hidden1, y, output)\n",
        "\n",
        "  gradients_global = tape.gradient(loss, model.layers[-1].trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients_global, model.layers[-1].trainable_variables))\n",
        "\n",
        "  gradients_local = tape.gradient(loss_local_hidden1, model.layers[-2].trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients_local, model.layers[-2].trainable_variables))\n",
        "\n",
        "  gradients_local = tape.gradient(loss_local_conv2,model.trainable_variables[10:17])\n",
        "  optimizer.apply_gradients(zip(gradients_local, model.trainable_variables[10:17]))\n",
        "\n",
        "  gradients_local = tape.gradient(loss_local_conv1,model.trainable_variables[2:10])\n",
        "  optimizer.apply_gradients(zip(gradients_local, model.trainable_variables[2:10]))\n",
        "\n",
        "  gradients_local = tape.gradient(loss_mask, model.layers[-15].trainable_variables)\n",
        "  optimizer_mask.apply_gradients(zip(gradients_local, model.layers[-15].trainable_variables))\n",
        "\n",
        "  train_acc(y,output)\n",
        "  train_loss(loss)\n",
        "\n",
        "  for i in range(3):\n",
        "    with tf.GradientTape(persistent =True) as tape:\n",
        "      hidden1_true_mask, hidden1_pred_mask, _ ,_,_,_= model([x,y], training=True)\n",
        "      loss_mask = compute_loss_mask(hidden1_true_mask, hidden1_pred_mask)\n",
        "    gradients_local = tape.gradient(loss_mask, model.layers[-15].trainable_variables)\n",
        "    optimizer_mask.apply_gradients(zip(gradients_local, model.layers[-15].trainable_variables))\n",
        "\n",
        "  mask_loss(loss_mask)\n",
        "\n",
        "def test_step(model, x, y):\n",
        "  _,_,_,_,_,output = model([x,y])\n",
        "  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  loss = scce(y, output)\n",
        "\n",
        "  test_loss(loss)\n",
        "  test_acc(y, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkO8Qh2Rkk8K"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N4JzYoTadFs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "!rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCWw43uk1uNo"
      },
      "outputs": [],
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + str(n_decision_makers)+'dropout/train'\n",
        "test_log_dir = 'logs/gradient_tape/' + current_time + str(n_decision_makers)+'dropout/test'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsQkc7yMaUM8"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/gradient_tape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNbzmKE54Ua1"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import time\n",
        "\n",
        "for epoch in range(0, 0+EPOCHS):\n",
        "  start=time.time()\n",
        "  for i, (train_x, train_y) in enumerate(tqdm(train_dataset)):\n",
        "    train_step(model, train_x, train_y, optimizer)\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', mask_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', train_acc.result(), step=epoch)\n",
        "\n",
        "  for test_x, test_y in test_dataset:\n",
        "    test_step(model, test_x, test_y)\n",
        "  with test_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', test_acc.result(), step=epoch)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, MaskLoss: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                         train_loss.result(),\n",
        "                         train_acc.result()*100,\n",
        "                         mask_loss.result(),\n",
        "                         test_loss.result(),\n",
        "                         test_acc.result()*100))\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  mask_loss.reset_states()\n",
        "  train_acc.reset_states()\n",
        "  test_acc.reset_states()\n",
        "\n",
        "  print(\"Time elapsed: \", time.time()-start)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import time\n",
        "\n",
        "for epoch in range(0, 0+EPOCHS):\n",
        "  start=time.time()\n",
        "  for i, (train_x, train_y) in enumerate(tqdm(train_dataset)):\n",
        "    train_step(model, train_x, train_y, optimizer)\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', mask_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', train_acc.result(), step=epoch)\n",
        "\n",
        "  for test_x, test_y in test_dataset:\n",
        "    test_step(model, test_x, test_y)\n",
        "  with test_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', test_acc.result(), step=epoch)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, MaskLoss: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                         train_loss.result(),\n",
        "                         train_acc.result()*100,\n",
        "                         mask_loss.result(),\n",
        "                         test_loss.result(),\n",
        "                         test_acc.result()*100))\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  mask_loss.reset_states()\n",
        "  train_acc.reset_states()\n",
        "  test_acc.reset_states()\n",
        "\n",
        "  print(\"Time elapsed: \", time.time()-start)"
      ],
      "metadata": {
        "id": "JSWulndyrph5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5hAE-qr0RZds"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}