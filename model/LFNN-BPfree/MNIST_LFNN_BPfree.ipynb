{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vroxxU1Mia53"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "def preprocess_images(images):\n",
        "  images = images.reshape((images.shape[0], 28,28)) / 255.\n",
        "  return images\n",
        "\n",
        "train_images = preprocess_images(train_images)\n",
        "test_images = preprocess_images(test_images)\n",
        "\n",
        "train_labels = np.expand_dims(train_labels,axis=-1)\n",
        "test_labels = np.expand_dims(test_labels,axis=-1)\n",
        "train_size = 60000\n",
        "batch_size = 200\n",
        "test_size = 10000\n",
        "\n",
        "train_images = tf.expand_dims(train_images, axis = -1)\n",
        "test_images = tf.expand_dims(test_images, axis = -1)"
      ],
      "metadata": {
        "id": "gWJyaObmidrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = (tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "                 .shuffle(train_size,reshuffle_each_iteration=True).batch(batch_size,drop_remainder=True))\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices((test_images,test_labels))\n",
        "                .shuffle(test_size).batch(batch_size,drop_remainder=True))"
      ],
      "metadata": {
        "id": "RLpHtoDrieVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.shape, train_images.shape"
      ],
      "metadata": {
        "id": "j1hklQlEifdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "cFhcjyaogR6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomDropout(tf.keras.layers.Layer):\n",
        "    def __init__(self, rate, input_dim, **kwargs):\n",
        "        super(CustomDropout, self).__init__(**kwargs)\n",
        "        self.rate = 1-rate\n",
        "        self.input_dim = input_dim\n",
        "        self.mask_w = self.add_weight(shape=(self.input_dim,n_decision_makers), trainable=True)\n",
        "        self.mask_b = self.add_weight(shape=(n_decision_makers,), initializer=\"zeros\",trainable=True)\n",
        "\n",
        "    def call(self, inputs, label, training=None):\n",
        "        if training:\n",
        "          scce = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
        "          loss = scce(tf.tile(tf.transpose([label],perm = [1,2,0]),[1,n_decision_makers,1]),inputs)\n",
        "          threshold = tfp.stats.percentile(loss, q=self.rate*100)\n",
        "          dropout_mask = (loss<=threshold) ## <= 1-rate keep the best 10%\n",
        "          mask = tf.tile(tf.expand_dims(dropout_mask, axis=-1), [1,1,10])\n",
        "\n",
        "          mask_pred = tf.nn.sigmoid(tf.matmul(tf.keras.layers.Flatten()(inputs), self.mask_w)+self.mask_b)\n",
        "          mask_pred = tf.tile(mask_pred, [1,10])\n",
        "          return tf.multiply(tf.keras.layers.Reshape((n_decision_makers,10))(mask_pred), inputs), tf.cast(mask,'float32'), mask_pred\n",
        "        else:\n",
        "          mask_pred = tf.nn.sigmoid(tf.matmul(tf.keras.layers.Flatten()(inputs), self.mask_w)+self.mask_b)\n",
        "          mask_pred = tf.tile(mask_pred, [1,10])\n",
        "          return tf.multiply(tf.keras.layers.Reshape((n_decision_makers,10))(mask_pred),inputs),tf.ones(shape = (batch_size,n_decision_makers,10)),mask_pred ## reshape self.mask"
      ],
      "metadata": {
        "id": "JR8nmWt3guXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_decision_makers = 4  #100\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self,**kwargs):\n",
        "      super(MyModel,self).__init__(**kwargs)\n",
        "\n",
        "      self.flat1 = tf.keras.layers.Flatten()\n",
        "      self.flat2 = tf.keras.layers.Flatten()\n",
        "      self.flat3 = tf.keras.layers.Flatten()\n",
        "      self.flat4 = tf.keras.layers.Flatten()\n",
        "      self.flat5 = tf.keras.layers.Flatten()\n",
        "      self.flat6 = tf.keras.layers.Flatten()\n",
        "      self.reshape1 = tf.keras.layers.Reshape((n_decision_makers,10))\n",
        "      self.reshape2 = tf.keras.layers.Reshape((n_decision_makers,10))\n",
        "      self.dropout1 = CustomDropout(0.7,n_decision_makers*10)\n",
        "\n",
        "      self.dropout4 = tf.keras.layers.Dropout(0.2)\n",
        "      self.dropout5 = tf.keras.layers.Dropout(0.2)\n",
        "\n",
        "      self.pool1 = tf.keras.layers.MaxPooling2D((2, 2))\n",
        "      self.pool2 = tf.keras.layers.MaxPooling2D((2, 2))\n",
        "\n",
        "      self.conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu',padding='same',kernel_regularizer=tf.keras.regularizers.l1(l=0.01),kernel_initializer='he_uniform',)\n",
        "      self.conv11 = tf.keras.layers.Conv2D(128, 3, activation='relu',padding='same',kernel_regularizer=tf.keras.regularizers.l1(l=0.01),kernel_initializer='he_uniform',)\n",
        "      self.dense1 = tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "      self.batchnorm1 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "      self.conv2 = tf.keras.layers.Conv2D(128, 3, activation='relu',padding='same',kernel_regularizer=tf.keras.regularizers.l1(l=0.01),kernel_initializer='he_uniform',)\n",
        "      self.conv22 = tf.keras.layers.Conv2D(64, 3, activation='relu',padding='same',kernel_regularizer=tf.keras.regularizers.l1(l=0.01),kernel_initializer='he_uniform',)\n",
        "      self.dense2 = tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "      self.batchnorm2 = tf.keras.layers.BatchNormalization()\n",
        "      self.dense5 = tf.keras.layers.Dense(n_decision_makers*10,activation=tf.nn.relu)\n",
        "\n",
        "      self.dense7 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\n",
        "    def call(self, input):\n",
        "\n",
        "      [input, label] = input\n",
        "      hidden_conv1 = self.dropout4(self.batchnorm1(self.pool1(self.conv1(self.conv11(input)))))\n",
        "      hidden_conv1_reshape = self.flat4(hidden_conv1)\n",
        "      hidden_conv1_out = self.dense1(hidden_conv1_reshape)\n",
        "\n",
        "      hidden_conv2 = self.dropout5(self.batchnorm2(self.pool2(self.conv2(self.conv22(hidden_conv1)))))\n",
        "      hidden_conv2_reshape = self.flat5(hidden_conv2)\n",
        "      hidden_conv2_out = self.dense2(hidden_conv2_reshape)\n",
        "\n",
        "      hidden1 = self.dense5(hidden_conv2_reshape)\n",
        "\n",
        "      hidden1_reshape = self.reshape1(hidden1)\n",
        "      hidden1_softmax = tf.nn.softmax(hidden1_reshape)\n",
        "      hidden1_out,hidden1_true_mask,hidden1_pred_mask = self.dropout1(hidden1_softmax,label)\n",
        "      outputs = self.dense7(self.flat1(hidden1_out)) #leader outputs\n",
        "\n",
        "      return self.flat2(hidden1_true_mask), hidden1_pred_mask, hidden_conv1_out, hidden_conv2_out, hidden1_out, outputs"
      ],
      "metadata": {
        "id": "Xrh9nKBQg-CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel()\n",
        "model([tf.zeros((batch_size, 28, 28, 1)),tf.zeros((batch_size, 1))])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "gZ4fQprxg_1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "local_loss"
      ],
      "metadata": {
        "id": "C8UDXjuSmqKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "\n",
        "train_loss, test_loss = tf.keras.metrics.Mean(),tf.keras.metrics.Mean()\n",
        "mask_loss = tf.keras.metrics.Mean()\n",
        "train_acc = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
        "test_acc = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')\n",
        "\n",
        "\n",
        "initial_learning_rate = 1e-4\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "optimizer_mask = tf.keras.optimizers.Adam()\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = 5e-4)\n",
        "\n",
        "\n",
        "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "cce = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def compute_loss(hidden1_true_mask, hidden1_pred_mask, conv1, conv2, hidden1, y, output):\n",
        "  loss_local_conv1 = scce(y,conv1)\n",
        "  loss_local_conv2 = scce(y,conv2)\n",
        "\n",
        "  loss_local_hidden1 = scce(tf.tile(y, [1,n_decision_makers]),hidden1)\n",
        "\n",
        "  loss = scce(y,output)\n",
        "\n",
        "  loss_mask = cce(hidden1_true_mask, hidden1_pred_mask)\n",
        "\n",
        "  return loss, loss_mask, loss_local_conv1, loss_local_conv2, loss_local_hidden1\n",
        "\n",
        "def compute_loss_mask(hidden1_true_mask, hidden1_pred_mask):\n",
        "  loss_mask = cce(hidden1_true_mask, hidden1_pred_mask)\n",
        "\n",
        "  return loss_mask\n",
        "\n",
        "def compute_acc(model, x, y):\n",
        "  _,_,_,_,_,output = model([x,y])\n",
        "  acc = tf.keras.metrics.sparse_categorical_accuracy(y, output)\n",
        "  return acc\n",
        "\n",
        "def train_step(model, x, y, optimizer):\n",
        "  with tf.GradientTape(persistent =True) as tape:\n",
        "    hidden1_true_mask, hidden1_pred_mask, conv1, conv2, hidden1, output = model([x,y], training=True)\n",
        "    loss, loss_mask, loss_local_conv1, loss_local_conv2, loss_local_hidden1 = compute_loss(hidden1_true_mask, hidden1_pred_mask, conv1, conv2, hidden1, y, output)\n",
        "\n",
        "  gradients_global = tape.gradient(loss, model.layers[-1].trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients_global, model.layers[-1].trainable_variables))\n",
        "\n",
        "  gradients_local = tape.gradient(loss_local_hidden1, model.layers[-2].trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients_local, model.layers[-2].trainable_variables))\n",
        "\n",
        "  gradients_local = tape.gradient(loss_local_conv2,model.trainable_variables[10:17])\n",
        "  optimizer.apply_gradients(zip(gradients_local, model.trainable_variables[10:17]))\n",
        "\n",
        "  gradients_local = tape.gradient(loss_local_conv1,model.trainable_variables[2:10])\n",
        "  optimizer.apply_gradients(zip(gradients_local, model.trainable_variables[2:10]))\n",
        "\n",
        "  gradients_local = tape.gradient(loss_mask, model.layers[-15].trainable_variables)\n",
        "  optimizer_mask.apply_gradients(zip(gradients_local, model.layers[-15].trainable_variables))\n",
        "\n",
        "  train_acc(y,output)\n",
        "  train_loss(loss)\n",
        "\n",
        "  for i in range(3):\n",
        "    with tf.GradientTape(persistent =True) as tape:\n",
        "      hidden1_true_mask, hidden1_pred_mask, _ ,_,_,_= model([x,y], training=True)\n",
        "      loss_mask = compute_loss_mask(hidden1_true_mask, hidden1_pred_mask)\n",
        "    gradients_local = tape.gradient(loss_mask, model.layers[-15].trainable_variables)\n",
        "    optimizer_mask.apply_gradients(zip(gradients_local, model.layers[-15].trainable_variables))\n",
        "\n",
        "  mask_loss(loss_mask)\n",
        "\n",
        "def test_step(model, x, y):\n",
        "  _,_,_,_,_,output = model([x,y])\n",
        "  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  loss = scce(y, output)\n",
        "\n",
        "  test_loss(loss)\n",
        "  test_acc(y, output)"
      ],
      "metadata": {
        "id": "sziwIIFXmpIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tko6VF29kaGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "5McPKfgmhb1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "\n",
        "!rm -rf ./logs/"
      ],
      "metadata": {
        "id": "bnocUEi_myqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + str(n_decision_makers)+'dropout/train'\n",
        "test_log_dir = 'logs/gradient_tape/' + current_time + str(n_decision_makers)+'dropout/test'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ],
      "metadata": {
        "id": "kyEt_DPXmzqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import time\n",
        "\n",
        "for epoch in range(0, 0+EPOCHS):\n",
        "  start=time.time()\n",
        "  for i, (train_x, train_y) in enumerate(tqdm(train_dataset)):\n",
        "    train_step(model, train_x, train_y, optimizer)\n",
        "\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', mask_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', train_acc.result(), step=epoch)\n",
        "\n",
        "  for test_x, test_y in test_dataset:\n",
        "    test_step(model, test_x, test_y)\n",
        "  with test_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', test_acc.result(), step=epoch)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, MaskLoss: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                         train_loss.result(),\n",
        "                         train_acc.result()*100,\n",
        "                         mask_loss.result(),\n",
        "                         test_loss.result(),\n",
        "                         test_acc.result()*100))\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  mask_loss.reset_states()\n",
        "  train_acc.reset_states()\n",
        "  test_acc.reset_states()\n",
        "\n",
        "  print(\"Time elapsed: \", time.time()-start)"
      ],
      "metadata": {
        "id": "qKu1yyD9m0v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xurA72Wqm2OR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}